<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Guangsheng Wu - Homepage</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.chinawgs.cn/"/>
  <updated>2019-06-18T10:53:53.094Z</updated>
  <id>https://www.chinawgs.cn/</id>
  
  <author>
    <name>Guangsheng Wu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>贝叶斯估计</title>
    <link href="https://www.chinawgs.cn/2019/06/18/be/"/>
    <id>https://www.chinawgs.cn/2019/06/18/be/</id>
    <published>2019-06-18T07:35:51.000Z</published>
    <updated>2019-06-18T10:53:53.094Z</updated>
    
    <content type="html"><![CDATA[<p>除了样本的信息，还有参数的先验信息。</p><a id="more"></a><p><strong>例1.</strong>  抛硬币试验。</p><p>设正面朝上的概率为$\theta$ ,假设未知参数$\theta$的先验概率服从$\beta$分布，记为$\theta \sim Be(\alpha, \beta)$，其概率密度是</p><script type="math/tex; mode=display">p(\theta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1}</script><p>不妨用$x_i=1$表示正面朝上，$x_i=0$表示反面朝上。<br>抛硬币服从二项分布$X \sim b(1,\theta)$，即$X$的分布律为：</p><script type="math/tex; mode=display">P\{X=x\}=p(x;\theta)=\theta^x (1-\theta)^{1-x},    x=0,1</script><p>因为此时的$\theta$为随机变量，所以$p(x;\theta)$应该看成给定$\theta$时$X$的条件概率，改用$p(x|\theta)$来表示，即$X$的分布律为：</p><script type="math/tex; mode=display">P\{X=x\}=p(x|\theta)=\theta^x (1-\theta)^{1-x},    x=0,1</script><p>已知先验信息$p(\theta)$，样本$X_1, X_2, \cdots, X_n$取到观察值$x_1, x_2, \cdots, x_n$，</p><p>参数$\theta$的后验概率为：</p><script type="math/tex; mode=display">\begin{align*}p(\theta|x_1,x_2,\cdots,x_n)&=\frac{p(\theta,x_1,x_2,\cdots,x_n)}{p(x_1,x_2,\cdots,x_n)}\\ & \color{red} {注：联合密度除以边际密度}\\&=\frac{p(\theta)p(x_1|\theta)p(x_2|\theta)\cdots p(x_n|\theta)}{\int p(\theta,x_1,x_2,\cdots,x_n)d\theta}\\& \color{red} {注：分母积分后与\theta无关}\\&= C \theta^{\alpha-1}(1-\theta)^{\beta-1}\Pi_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i}\\& \color{red} {注：C是一个与\theta无关的常数}\\&=C\theta^{\alpha -1 + \Sigma_{i=1}^n x_i}(1-\theta)^{\beta-1+n- \Sigma_{i=1}^n x_i}\end{align*}</script><p>令$L(\theta)=p(\theta|x_1,x_2, \cdots, x_n)$</p><script type="math/tex; mode=display">\begin{align*}\ln L(\theta) &= \ln C +\ln[\theta^{\alpha -1 + \Sigma_{i=1}^n x_i}(1-\theta)^{\beta-1+n- \Sigma_{i=1}^n x_i}]\\&= \ln C +(\alpha-1+\sum_{i=1}^n x_i)\ln\theta + (\beta-1+n-\sum_{i=1}^n x_i)\ln(1-\theta) \end{align*}</script><p>令$\frac{\partial \ln L(\theta)}{\partial \theta} =0$</p><script type="math/tex; mode=display">\frac{\alpha-1+\sum_{i=1}^n x_i}{\theta}-\frac{\beta-1+n-\sum_{i=1}^n x_i}{1-\theta}=0\\\alpha-1+\sum_{i=1}^n x_i=\theta(n+\alpha+\beta-2)\\\hat \theta = \frac{\alpha-1+\sum_{i=1}^n x_i}{n+\alpha+\beta-2}</script><p>当$n \to \infty$时，$\hat \theta \to \frac{1}{n} \sum_{i=1}^n x_i= \overline x$，此时正好是最大似然估计。</p><p>当$n=1$时，只有一个样本（$x_i=0或1$），</p><p>$\hat \theta=\frac{\alpha-1}{\alpha+\beta-1}$,  ($x_i=0$)；</p><p>$\hat \theta=\frac{\alpha}{\alpha+\beta-1}$,  ($x_i=1$)。</p><p><br></p><p><strong>例2.</strong>  推导下列正态分布均值$\mu$的贝叶斯估计。</p><p>样本数据$x_1,x_2,\cdots,x_n$来自正态分布$N(\mu,\sigma^2)$，其中$\mu$未知，$\sigma$已知。假设$\mu$的先验分布为正态分布$\mu \sim N(0, \tau^2)$ ，根据样本$x_1,x_2,\cdots,x_n$写出$\mu$的贝叶斯估计。</p><script type="math/tex; mode=display">\begin{align*}p(\mu|x_1,x_2,\cdots,x_n)&=\frac{p(\mu,x_1,x_2,\cdots,x_n)}{p(x_1,x_2,\cdots,x_n)}\\&= \frac{p(\mu)p(x_1,x_2,\cdots,x_n|\mu)}{p(x_1,x_2,\cdots,x_n)}\\&= \frac{p(\mu)p(x_1|\mu)p(x_2|\mu)\cdots p(x_n|\mu)}{\int p(\mu,x_1,x_2,\cdots,x_n) \mathrm{d} \mu}\\&= \frac{ \frac{1}{\sqrt{2\pi}\tau}\exp(-\frac{\mu^2}{2\tau^2})\Pi_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x_i-\mu)^2}{2\sigma^2}) } {\int p(\mu,x_1,x_2,\cdots,x_n) \mathrm{d} \mu} \\&= C \exp(-\frac{\mu^2}{2\tau^2}) \Pi_{i=1}^n \exp(-\frac{(x_i-\mu)^2}{2\sigma^2})\\& \color{red} {注：C是一个与\mu无关的常数}\\\end{align*}</script><p> 令$L(\mu)=p(\mu|x_1,x_2,\cdots,x_n)$</p><script type="math/tex; mode=display">\begin{align*}\ln L(\mu)=\ln C - \frac{\mu^2}{2\tau^2}-\sum_{i=1}^n \frac{(x_i-\mu)^2}{2\sigma^2}\end{align*}</script><p>令$ \frac{\partial \ln L(\mu)}{\partial \mu}=0$</p><script type="math/tex; mode=display">-\frac{\mu}{\tau^2}+\sum_{i=1}^n \frac{(x_i-\mu)}{\sigma^2}=0\\\tau^2 \sum_{i=1}^n (x_i-\mu)=\sigma^2 \mu\\\tau^2 \sum_{i=1}^n x_i = (n \tau^2 + \sigma^2)\mu\\\hat \mu = \frac{\sum_{i=1}^n x_i}{n+ \frac{\sigma^2}{\tau^2}}</script><p>当$n \to \infty$时，$\hat \theta \to \frac{1}{n}\sum_{i=1}^n x_i = \overline x$，此时正好是最大似然估计。</p><p><br></p><p><strong>总结：</strong></p><p>问题：已知样本集$D=\{x_1,x_2, \cdots, x_n\}$服从某个概率分布，但是参数$\theta$未知。</p><p>最大似然估计</p><p>（1）参数$\theta$是一个未知的定值</p><p>（2）目标：找到一个参数$\theta$，使得样本集$D$发生的概率最大</p><script type="math/tex; mode=display">\max \quad p(D|\theta)</script><p>贝叶斯估计</p><p>（1）参数$\theta$是未知的随机变量，本身服从一定的概率分布（先验分布）</p><p>（2）目标：样本集$D$发生的情况下，哪一个$\theta$发生的概率最大</p><script type="math/tex; mode=display">\max \quad p(\theta|D)</script><p><br></p><p>贝叶斯估计求解的一般步骤：</p><p>（1）确定参数$\theta$的先验分布$p(\theta)$ </p><p>（2）由样本集$D=\{x_1,x_2, \cdots, x_n\}$ 求出样本的联合分布$p(D|\theta)$，它是$\theta$的函数：</p><script type="math/tex; mode=display">p(D|\theta)= \Pi_{i=1}^n p(x_i|\theta)</script><p>（3）利用贝叶斯公式，求$\theta$的后验分布：</p><script type="math/tex; mode=display">L(\theta)=p(\theta|D)=\frac{p(\theta,D)}{p(D)}=\frac{p(D|\theta)p(\theta)}{\int p(\theta,D)\mathrm{d} \theta}</script><p>（4）取对数    $\ln L(\theta)$</p><p>（5）求偏导    $\frac{\partial \ln L(\theta)}{\partial \theta}$</p><p>（6）解方程（组）  $\frac{\partial \ln L(\theta)}{\partial \theta}=0$，求解出$\hat \theta$ </p><p><br></p><p><strong>参考文献：</strong></p><p>[1] 邰淑彩, 孙韫玉, 何娟娟.  应用数理统计（第二版）[M].  武汉: 武汉大学出版社, 2005.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;除了样本的信息，还有参数的先验信息。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://www.chinawgs.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://www.chinawgs.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率统计" scheme="https://www.chinawgs.cn/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯公式</title>
    <link href="https://www.chinawgs.cn/2019/06/11/bayes/"/>
    <id>https://www.chinawgs.cn/2019/06/11/bayes/</id>
    <published>2019-06-11T06:47:06.000Z</published>
    <updated>2019-06-11T13:52:49.530Z</updated>
    
    <content type="html"><![CDATA[<h2 id="条件概率"><a href="#条件概率" class="headerlink" title="条件概率"></a>条件概率</h2><p>条件概率的定义：设$A$, $B$是两个事件，且$P(A)&gt;0$，称</p><script type="math/tex; mode=display">P(B|A)=\frac{P(AB)}{P(A)}</script><p>为在事件$A$发生的条件下，事件$B$发生的<strong>条件概率</strong>。</p> <a id="more"></a><p>同理，可得出在事件$B$发生的条件下，事件$A$发生的条件概率为</p><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/assets/bayes/tiaojian.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>根据文氏图可以看出，在事件$A$发生的情况下，事件$B$发生的概率，就是$P(A \cap B)$除以$P(A)$，即$P(AB)$除以$P(A)$。</p><h2 id="乘法定理"><a href="#乘法定理" class="headerlink" title="乘法定理"></a>乘法定理</h2><p>设$P(A) &gt; 0$，则有</p><script type="math/tex; mode=display">P(AB)=P(B|A)P(A)</script><p>由条件概率的定义，可得到上述乘法定理。</p><p>同理，可得：</p><script type="math/tex; mode=display">P(AB)=P(A|B)P(B)</script><p><br></p><p>$P(AB)$又叫<strong>联合概率</strong>（两个事件共同发生的概率），还可以记为$P(A \cap B)$或者$P(A,B)$</p><h2 id="样本空间及其划分"><a href="#样本空间及其划分" class="headerlink" title="样本空间及其划分"></a>样本空间及其划分</h2><p>定义：设$S$为试验$E$的样本空间，$B_1,B_2,\cdots,B_n$为$E$的一组事件。若</p><p>(1) $B_i B_j = \emptyset$，  $i \neq j$，   $ i,j=1,2, \cdots, n$；</p><p>(2) $B_1 \cup B_2 \cup \cdots \cup B_n = S$，</p><p>则称$B_1 , B_2 , \cdots , B_n $为样本空间$S$的一个划分。</p><p>若$B_1 , B_2 , \cdots , B_n $为样本空间$S$的一个划分，那么，对于每次试验，事件$B_1 , B_2 , \cdots , B_n $中必有一个且仅有一个发生。</p><h2 id="全概率公式"><a href="#全概率公式" class="headerlink" title="全概率公式"></a>全概率公式</h2><p>定理：设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1 , B_2 , \cdots , B_n$ 为$S$的一个划分，且$P(B_i)&gt;0$ （$i=1,2, \cdots, n$），则</p><script type="math/tex; mode=display">P(A)=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+ \cdots +P(A|B_n)P(B_n)</script><p>称为全概率公式。</p><p>特别地，当$n=2$，并将$B_1$记为$B$，此时$B_2$就是$\overline B$， 全概率公式变为</p><script type="math/tex; mode=display">P(A)=P(A|B)P(B)+P(A|\overline B)P(\overline B)</script><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/assets/bayes/quangailv.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><p>定理：设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1 , B_2 , \cdots , B_n$ 为$S$的一个划分，且$P(A)&gt;0$，$P(B_i)&gt;0$ （$i=1,2, \cdots, n$），则</p><script type="math/tex; mode=display">P(B_i|A)=\frac{P(A|B_i)P(B_i)}{\sum_{j=1}^n P(A|B_j)P(B_j)}, \quad i=1,2, \cdots,n</script><p>称为贝叶斯公式。</p><p>证明：由条件概率的定义及全概率公式即得</p><script type="math/tex; mode=display">P(B_i|A)=\frac{P(B_iA)}{P(A)}=\frac{P(A|B_i)P(B_i)}{\sum_{j=1}^n P(A|B_j)P(B_j)}, \quad i=1,2, \cdots,n</script><p><br></p><p>特别地，当$n=2$，并将$B_1$记为$B$，此时$B_2$就是$\overline B$， 贝叶斯公式变为</p><script type="math/tex; mode=display">P(B|A)=\frac{P(AB)}{P(A)}=\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\overline B)P(\overline B)}</script><p>其中，$P(B)$称为$B$的<strong>先验概率</strong>，即在事件$A$发生之前，我们对$B$事件发生概率的一个基本判断，往往是基于以往的数据分析得到的。$P(B|A)$称为$B$的<strong>后验概率</strong>，即在$A$事件发生之后，我们对$B$事件发生的概率进行重新评估。</p><p><br></p><p>$A$和$B$只是表示事件的符号而已，类似地，贝叶斯公式也可被写为：</p><script type="math/tex; mode=display">P(A|B)=\frac{P(AB)}{P(B)}=\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\overline A)P(\overline A)}</script><p>此时，$P(A)$称为$A$的<strong>先验概率</strong>，即在事件$B$发生之前，我们对$A$事件发生概率的一个基本判断，往往是基于以往的数据分析得到的。$P(A|B)$称为$A$的<strong>后验概率</strong>，即在$B$事件发生之后，我们对$A$事件发生的概率进行重新评估。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p>例1. 已知人群中某种疾病的发病率为0.001。现有一种试剂可以检验是否患有该疾病，在患病情况下，有99%的可能呈现阳性。没患病情况下，也有5%的可能呈现阳性。现有一个人被检出阳性，请问他确实患有该病的可能性有多大？</p><p>解：以$A$表示事件“检验结果呈现阳性”，$B$表示事件“患病”。</p><p>已知$P(B)=0.001$，这是先验概率，即没有检查之前，预计的患病可能性，根据人群的历史发病数据得来的。</p><p>现在要求$P(B|A)$，就是后验概率，即检查为阳性之后患病的可能性。</p><script type="math/tex; mode=display">\begin{align*}P(B|A) &= \frac{P(AB)}{P(A)}  \\&= \frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\overline B)P(\overline B)}  \\&= \frac{0.99 \times 0.001}{0.99 \times 0.001 + 0.05 \times 0.999} \\& \approx 0.019\end{align*}</script><p><strong>参考文献：</strong></p><p>[1] 盛骤, 谢式千, 潘承毅.  概率论与数理统计（第四版）[M].  北京: 高等教育出版社, 2008.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;条件概率&quot;&gt;&lt;a href=&quot;#条件概率&quot; class=&quot;headerlink&quot; title=&quot;条件概率&quot;&gt;&lt;/a&gt;条件概率&lt;/h2&gt;&lt;p&gt;条件概率的定义：设$A$, $B$是两个事件，且$P(A)&amp;gt;0$，称&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(B|A)=\frac{P(AB)}{P(A)}&lt;/script&gt;&lt;p&gt;为在事件$A$发生的条件下，事件$B$发生的&lt;strong&gt;条件概率&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://www.chinawgs.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="概率统计" scheme="https://www.chinawgs.cn/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析（PCA）</title>
    <link href="https://www.chinawgs.cn/2019/06/04/pca/"/>
    <id>https://www.chinawgs.cn/2019/06/04/pca/</id>
    <published>2019-06-04T15:10:06.000Z</published>
    <updated>2019-06-04T16:01:38.304Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-主成分分析"><a href="#1-主成分分析" class="headerlink" title="1. 主成分分析"></a>1. 主成分分析</h2><p>优点：降低数据的复杂性，识别最重要的多个特征。</p><p>缺点：不一定需要，且可能损失有用信息。</p><p>使用数据类型：数值型数据</p> <a id="more"></a><h2 id="2-在NumPy中实现PCA"><a href="#2-在NumPy中实现PCA" class="headerlink" title="2. 在NumPy中实现PCA"></a>2. 在NumPy中实现PCA</h2><p>将数据转换成前K个主成分的伪码：</p><p>去除平均值（让样本矩阵中心化，即每一个维度减去该维度的均值，使每一维度上的均值为0）</p><p>计算协方差矩阵</p><p>计算协方差矩阵的特征值和特征向量</p><p>将特征值从大到小排序</p><p>保留最上面的K个特征向量</p><p>将数据转换到上述K个特征向量构建的新空间中</p><p>补充阅读：协方差矩阵</p><p>（1）一个维度上方差的定义：</p><script type="math/tex; mode=display">var(X)=\frac{\sum_{i=1}^{n}(X_{i}-{\bar{X}})(X_{i}-{\bar{X}})}{n-1}</script><p>（2）协方差的定义：</p><script type="math/tex; mode=display">cov(X,Y)=\frac{\sum_{i=1}^{n}(X_{i}-{\bar{X}})(Y_{i}-{\bar{Y}})}{n-1}</script><p>方差是协方差的一个特例，X与其自身的协方差就是X的方差。</p><p>分母为n-1是因为随机变量的数学期望未知，以样本均值代替，自由度减一。</p><p>协方差就是定义了两个维度之间的相关性，即这个样本的两个维度之间有没有关系。</p><p>协方差为0，证明这两个维度之间没有关系；协方差为正，两个维度正相关；协方差为负，两个维度负相关。</p><p>（3）协方差矩阵：</p><p>对n个维度，任意两个维度都计算一个协方差，组成矩阵。</p><script type="math/tex; mode=display">C_{n \times n}=( c_{i,j}, c_{i,j}=cov(Dim_{i},Dim_{j}))</script><p>直观地，对于一个含有x，y，z三个维度的样本，协方差矩阵如下：</p><script type="math/tex; mode=display">C=\begin{pmatrix}cov(x,x) & cov(x,y) & cov(x,z)\\ cov(y,x) & cov(y,y) & cov(y,z)\\ cov(z,x) & cov(z,y) & cov(z,z)\end{pmatrix}</script><p>可以看出，对角线表示了样本在各个维度上的方差，其他元素表示不同维度两两之间的相关性。</p><p>例：</p><p>假设原始数据为5个样本，每个样本有2个特征x和y。矩阵是：</p><script type="math/tex; mode=display">Data =\begin{bmatrix}1 & -1\\ 1 & 1\\ 2 & 1\\ 2 & 2\\ 4 & 2\end{bmatrix}</script><p>第一步，样本矩阵中心化（计算每个维度的均值，所有样本都减去均值）</p><p>维度x的均值为2，维度y的均值为1。</p><p>各维度去均值后，矩阵是：</p><script type="math/tex; mode=display">DataAdjust = \begin{bmatrix}-1 & -2\\ -1 & 0\\ 0 & 0\\ 0 & 1\\ 2 & 1\end{bmatrix}</script><p>这时，两个维度上的均值都为0。</p><p>第二步，计算协方差矩阵</p><p>协方差矩阵：</p><script type="math/tex; mode=display">\begin{align*}C &=\begin{bmatrix}cov(x,x) & cov(x,y)\\ cov(y,x) & cov(y,y)\end{bmatrix}\\&=\begin{bmatrix}\frac{(-1) \times (-1) + (-1) \times (-1) + 0 \times 0 + 0 \times 0 + 2 \times 2}{4} & \frac{(-1) \times (-2) + (-1) \times (0) + 0 \times 0 + 0 \times 1 + 2 \times 1}{4}\\ \frac{(-2) \times (-1) + (0) \times (-1) + 0 \times 0 + 1 \times 0 + 1 \times 2}{4}  & \frac{(-2) \times (-2) + (0) \times (0) + 0 \times 0 + 1 \times 1 + 1 \times 1}{4}\end{bmatrix}\\&=\begin{bmatrix}1.5 & 1.0\\ 1.0 & 1.5\end{bmatrix}\end{align*}</script><p>第三步，计算协方差矩阵的特征值和特征向量</p><script type="math/tex; mode=display">eigenValues=\begin{pmatrix}2.5 & 0.5\end{pmatrix}</script><script type="math/tex; mode=display">eigenVectors=\begin{pmatrix}0.70710678 & -0.70710678\\0.70710678 & 0.70710678\end{pmatrix}</script><p>特征值2.5对应的特征向量是eigenVectors的第1列：</p><script type="math/tex; mode=display">\begin{pmatrix}0.70710678\\0.70710678\end{pmatrix}</script><p>特征值0.5对应的特征向量是eigenVectors的第2列：</p><script type="math/tex; mode=display">\begin{pmatrix}-0.70710678\\0.70710678\end{pmatrix}</script><p>第四步，将特征值按照从大到小的顺序排列，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵</p><p>这里特征值只有2个，我们选取最大的那个2.5，对应的特征向量是：</p><script type="math/tex; mode=display">\begin{pmatrix}0.70710678\\0.70710678\end{pmatrix}</script><p>第五步，将样本点投影到选取的特征向量上。</p><p>假设样本数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m×n)，协方差矩阵为C(n×n)，选取的k个特征向量组成的矩阵为TopEigenVectors(n×k)，那么投影后的低维数据LowDimData为：</p><script type="math/tex; mode=display">LowDimData_{m\times k}=DataAdjust_{m \times n} \times TopEigenVectors_{n \times k}</script><p>这里是</p><script type="math/tex; mode=display">LowDimData=\begin{bmatrix}-1 & -2\\ -1 & 0\\ 0 & 0\\ 0 & 1\\ 2 & 1\end{bmatrix} \times \begin{pmatrix}0.70710678\\0.70710678\end{pmatrix}=\begin{bmatrix}-2.12132034\\ -0.70710678\\ 0\\ 0.70710678\\ 2.12132034\end{bmatrix}</script><p>这样就将原始样本的n维（这个例子n=2）特征变成了k维（这个例子k=1），这k维就是原始特征在k维上的投影。</p><p>第六步，查看重构之后的数据，用于调试（假设K取值为n，重构回去的数据reconMat应该与原始数据Data重合）</p><script type="math/tex; mode=display">reconMat = (LowDimData * TopEigenVectors.T) + meanVals</script><p>程序清单 PCA算法</p><pre><code class="lang-python">from numpy import *import matplotlibimport matplotlib.pyplot as pltdef loadDataSet(fileName, delim=&#39;\t&#39;):    fr = open(fileName)    stringArr = [line.strip().split(delim) for line in fr.readlines()]    datArr = [list(map(float,line)) for line in stringArr]    # python3 needs list()    return mat(datArr)def pca(dataMat, topNfeat=9999999):    meanVals = mean(dataMat, axis=0)    dataAdjust = dataMat - meanVals #remove mean    covMat = cov(dataAdjust, rowvar=0)    eigVals,eigVects = linalg.eig(mat(covMat))    eigValInd = argsort(eigVals)            #sort, sort goes smallest to largest    eigValInd = eigValInd[:-(topNfeat+1):-1]  #cut off unwanted dimensions    topEigVects = eigVects[:,eigValInd]       #reorganize eig vects largest to smallest    lowDimDataMat = dataAdjust * topEigVects#transform data into new dimensions    reconMat = (lowDimDataMat * topEigVects.T) + meanVals    return lowDimDataMat, reconMatdataMat = loadDataSet(&#39;testSet.txt&#39;)lowDimDataMat, reconMat = pca(dataMat, 1)fig = plt.figure()ax = fig.add_subplot(111)ax.scatter(dataMat[:,0].flatten().A[0],  dataMat[:,1].flatten().A[0], marker=&#39;^&#39;, s=90)ax.scatter(reconMat[:,0].flatten().A[0], reconMat[:,1].flatten().A[0], marker=&#39;o&#39;, s=50, c=&#39;red&#39;)plt.show()</code></pre><p>运行结果如图1：<br><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="/assets/pca/Figure_1.png" alt title>                </div>                <div class="image-caption"></div>            </figure></p><p><center> <font size="2" color="gray">图1 原始数据集(三角形)及第一主成分(圆形)</font>  </center><br>用到的数据集<a href="/assets/pca/testSet.txt">testSet.txt</a></p><p><strong>参考文献：</strong></p><p>[1]  (美) Harrington Peter著.  机器学习实战[M]. 李锐, 李鹏, 曲亚东, 王斌, 译.  北京: 人民邮电出版社, 2013.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-主成分分析&quot;&gt;&lt;a href=&quot;#1-主成分分析&quot; class=&quot;headerlink&quot; title=&quot;1. 主成分分析&quot;&gt;&lt;/a&gt;1. 主成分分析&lt;/h2&gt;&lt;p&gt;优点：降低数据的复杂性，识别最重要的多个特征。&lt;/p&gt;
&lt;p&gt;缺点：不一定需要，且可能损失有用信息。&lt;/p&gt;
&lt;p&gt;使用数据类型：数值型数据&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://www.chinawgs.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://www.chinawgs.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>最大似然估计(MLE)</title>
    <link href="https://www.chinawgs.cn/2019/06/02/mle/"/>
    <id>https://www.chinawgs.cn/2019/06/02/mle/</id>
    <published>2019-06-02T10:23:12.000Z</published>
    <updated>2019-06-14T08:48:13.912Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是最大似然估计？"><a href="#什么是最大似然估计？" class="headerlink" title="什么是最大似然估计？"></a>什么是最大似然估计？</h2><p>最大似然估计（Maximum Likelihood Estimation）是一种参数估计的方法。其核心思想是：找到参数$\theta$的一个估计值，使得当前样本出现的可能性最大。</p> <a id="more"></a><p>设总体$X$属于离散型，其分布律$P\{ X=x\}=p(x;\theta), \theta \in \Theta$的形式为已知，$\theta$为待估参数，$\Theta$是$\theta$可能取值的范围。设$X_1,X_2,\cdots,X_n$是来自$X$的样本，则$X_1,X_2,\cdots,X_n$的联合分布律为：</p><script type="math/tex; mode=display">\prod_{i=1}^{n}p(x_i;\theta)</script><p>样本$X_1,X_2,\cdots,X_n$取到观察值$x_1,x_2,\cdots,x_n$的概率，即事件$\{X_1=x_1,X_2=x_2,\cdots,X_n=x_n\}$发生的概率为：</p><script type="math/tex; mode=display">L(\theta)=L(x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta),\theta\in\Theta</script><p>这一概率随$\theta$的取值而变化，它是$\theta$的函数，$L(\theta)$称为样本的<strong>似然函数</strong>。</p><p>最大似然估计法，就是固定样本观察值$x_1,x_2,\cdots,x_n$，在参数$\theta$取值的可能范围$\Theta$内挑选出一个$\hat\theta$，使得似然函数$L(x_1,x_2,\cdots,x_n;\theta)$达到最大。即：</p><script type="math/tex; mode=display">\hat\theta=\mathop{\arg\max}_{\theta\in\Theta}L(x_1,x_2,\cdots,x_n;\theta)</script><p>这样得到的$\hat\theta$与样本值$x_1,x_2,\cdots,x_n$有关，常记为$\hat\theta(x_1,x_2,\cdots,x_n)$，称为参数$\theta$的<strong>最大似然估计值</strong>，而相应的统计量$\hat\theta(X_1,X_2,\cdots,X_n)$称为参数$\theta$的<strong>最大似然估计量</strong>。</p><h2 id="为什么要有参数估计？"><a href="#为什么要有参数估计？" class="headerlink" title="为什么要有参数估计？"></a>为什么要有参数估计？</h2><p>当模型已定，参数未知时。</p><p>例如，假设我们知道全国人民的身高服从正态分布，但不知道均值和方差。这时可以通过采样，观察其结果，然后再用样本数据的结果推出正态分布的均值与方差的最大概率值，这样就可以知道全国人民的身高分布的函数。</p><h2 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h2><p><span>1. </span> 抛硬币。现有一个正反面不是很均匀的硬币，如果正面朝上记为H，反面朝上记为T，抛10次的结果如下：</p><p>   T, H, T, T, H, T, T, T, H, T</p><p>   求这个硬币正面朝上的概率有多大？</p><p>   很显然，这个概率是0.3。现在用MLE的思想来求解它。</p><p>   设$x_1,x_2,\cdots,x_n$是相应于样本$X_1,X_2,\cdots,X_n$的一个样本值。</p><p>   不妨用$x_i=1$表示正面朝上，$x_i=0$表示反面朝上</p><p>   设正面朝上的概率为$\theta$，抛硬币服从二项分布$X \sim b(1,\theta)$，即$X$的分布律为：</p><script type="math/tex; mode=display">P\{X=x\}=p(x;\theta)=\theta^x (1-\theta)^{1-x},    x=0,1</script><p>   似然函数为：</p><script type="math/tex; mode=display">   L(\theta)=\prod_{i=1}^{n}p(x_i;\theta)=\prod_{i=1}^{n}\theta^{x_i}(1-\theta)^{1-x_i}</script><p>   取对数后，为</p><script type="math/tex; mode=display">\begin{align*}   \ln L(\theta)&=\ln \prod_{i=1}^{n}\theta^{x_i}(1-\theta)^{1-x_i}\\   &=\sum_{i=1}^{n}\ln[\theta^{x_i}(1-\theta)^{1-x_i}]\\   &=\sum_{i=1}^{n}[\ln \theta^{x_i}+\ln(1-\theta)^{1-x_i}]\\   &=\sum_{i=1}^{n}[x_i \ln \theta+(1-x_i)\ln(1-\theta)]\\   &=\sum_{i=1}^{n}x_i \ln \theta+(n-\sum_{i=1}^{n}x_i)\ln(1-\theta)   \end{align*}</script><p>   求导：</p><script type="math/tex; mode=display">\frac{\partial \ln L(\theta)}{\partial \theta}=\frac{\sum_{i=1}^{n} x_i}{\theta}-\frac{n-\sum_{i=1}^{n} x_i}{1-\theta}</script><p>   令$\frac{\partial \ln L(\theta)}{\partial \theta}=0$，可得：</p><script type="math/tex; mode=display">\hat \theta = \frac{\sum_{i=1}^{n} x_i}{n}</script><p>   可知概率$\hat \theta=0.3$</p><p><span>2. </span> 设$X \sim N(\mu, \sigma^2)$, $\mu, \sigma^2$为未知参数，$x_1,x_2,\cdots,x_n$是来自$X$的一个样本值。求$\mu, \sigma^2$的最大似然估计量。</p><p>   解：$X$的概率密度为：</p><script type="math/tex; mode=display">f(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}exp[-\frac{1}{2\sigma^2}(x-\mu)^2]</script><p>   似然函数为：</p><script type="math/tex; mode=display">   \begin{align*}   L(\mu,\sigma^2)&=\prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma}exp[-\frac{1}{2\sigma^2}(x_i-\mu)^2]\\   &=(\frac{1}{\sqrt{2\pi}\sigma})^n exp[-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2]\\   &=(\frac{1}{2\pi \sigma^2})^{\frac{n}{2}} exp[-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2]\\   &=(2\pi)^{-\frac{n}{2}}(\sigma^2)^{-\frac{n}{2}}exp[-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2]   \end{align*}</script><p>   它的对数：</p><script type="math/tex; mode=display">\ln L(\mu,\sigma^2)=-\frac{n}{2}\ln(2\pi)-\frac{n}{2}\ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2</script><p>   令</p><script type="math/tex; mode=display">\left\{       \begin{array}{l}           \frac{\partial \ln L(\mu,\sigma^2)}{\partial \mu}=\frac{1}{\sigma^2}\Sigma_{i=1}^n(x_i-\mu)=0\\           \frac{\partial \ln L(\mu,\sigma^2)}{\partial \sigma^2}= -\frac{n}{2\sigma^2} + \frac{1}{2 \sigma^4}\sum_{i=1}^n (x_i-\mu)^2=0       \end{array}   \right.</script><p>   联合求解，得到参数$\mu$和$\sigma^2$的最大似然估计值分别为：</p><script type="math/tex; mode=display">   \left\{       \begin{array}{l}           \hat \mu = \overline x = \frac{1}{n} \sum_{i=1}^n x_i\\           \hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i-\overline x)^2       \end{array}   \right.</script><p>   相应的最大似然估计量分别为：    $\hat \mu=\bar X$,       $\hat \sigma^2=\frac{1}{n}\sum_{i=1}^n (X_i- \overline X)$</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>求最大似然估计值的一般步骤：</p><p>（1）写出总体$X$的分布律$p(x; \theta)$（$X$为离散型随机变量）或者概率密度$f(x; \theta)$（$X$为连续型随机变量）</p><p>（2）写出样本的似然函数$L(\theta)$ </p><p>离散型：$L(\theta)=L(x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta)$<br>连续型：$L(\theta)=L(x_1,x_2,\cdots,x_n;\theta)=\prod_{i=1}^{n}f(x_i;\theta)$</p><p>（3）对似然函数取对数     $\ln L(\theta)$</p><p>（4）求偏导     $\frac{\partial \ln L(\theta)}{\partial \theta}$</p><p>（5）解方程（组） $\frac{\partial \ln L(\theta)}{\partial \theta}=0$，得到参数$\theta$的最大似然估计值$\hat \theta$</p><p>注：参数可能是一个（如例1，只有一个参数$\theta$），也可能是一组（如例2，有两个参数：$\mu, \sigma^2$），一组参数时，求解方法类似。</p><p><br></p><p><strong>参考文献：</strong></p><p>[1] 盛骤, 谢式千, 潘承毅.  概率论与数理统计（第四版）[M].  北京: 高等教育出版社, 2008.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是最大似然估计？&quot;&gt;&lt;a href=&quot;#什么是最大似然估计？&quot; class=&quot;headerlink&quot; title=&quot;什么是最大似然估计？&quot;&gt;&lt;/a&gt;什么是最大似然估计？&lt;/h2&gt;&lt;p&gt;最大似然估计（Maximum Likelihood Estimation）是一种参数估计的方法。其核心思想是：找到参数$\theta$的一个估计值，使得当前样本出现的可能性最大。&lt;/p&gt;
    
    </summary>
    
      <category term="学习笔记" scheme="https://www.chinawgs.cn/categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    
    
      <category term="机器学习" scheme="https://www.chinawgs.cn/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="概率统计" scheme="https://www.chinawgs.cn/tags/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://www.chinawgs.cn/2019/06/02/hello-world/"/>
    <id>https://www.chinawgs.cn/2019/06/02/hello-world/</id>
    <published>2019-06-02T05:20:10.000Z</published>
    <updated>2019-06-03T05:52:32.144Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br> <a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="lang-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="lang-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="lang-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="lang-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="测试" scheme="https://www.chinawgs.cn/categories/%E6%B5%8B%E8%AF%95/"/>
    
    
      <category term="杂谈" scheme="https://www.chinawgs.cn/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
</feed>
