{"meta":{"title":"Guangsheng Wu - Homepage","subtitle":null,"description":"吴光生的个人主页, Homepage of Guangsheng Wu","author":"Guangsheng Wu<br>吴光生","url":"https://www.chinawgs.cn","root":"/"},"pages":[{"title":"About me","date":"2019-06-02T10:34:45.000Z","updated":"2019-06-20T11:23:20.614Z","comments":true,"path":"about/index.html","permalink":"https://www.chinawgs.cn/about/index.html","excerpt":"","text":""},{"title":"个人简历（Resume）","date":"2019-06-02T10:34:45.000Z","updated":"2019-06-12T02:58:42.888Z","comments":true,"path":"resume/index.html","permalink":"https://www.chinawgs.cn/resume/index.html","excerpt":"","text":"张三(test@XXX.com)个人信息 本科/XXX大学(20XX.9-20XX.7)/计算机科学与技术 工作年限：2年 技术博客：http://xxxxx.com 地点：北京 工作经历五道口宇宙中心XXXX项目（2013.10-至今） XXXXXXXXXXXXXXXXX XXXXXXXXXXXXXXXXXX XXXXXXX XXXXXXXXXXXXXXXXXXXXXX 技能列表熟悉：Android/Java了解：C#/WP，Python，HTML, Markdown等","author":"吴光生"},{"title":"categories","date":"2019-06-02T10:33:28.000Z","updated":"2019-06-02T10:33:28.727Z","comments":true,"path":"categories/index - 副本.html","permalink":"https://www.chinawgs.cn/categories/index - 副本.html","excerpt":"","text":""},{"title":"","date":"2019-06-03T03:35:31.483Z","updated":"2019-06-03T03:35:31.483Z","comments":false,"path":"categories/index.html","permalink":"https://www.chinawgs.cn/categories/index.html","excerpt":"","text":""},{"title":"","date":"2019-06-11T14:52:08.492Z","updated":"2019-06-02T10:32:42.659Z","comments":false,"path":"tags/index.html","permalink":"https://www.chinawgs.cn/tags/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-06-02T10:30:07.000Z","updated":"2019-06-02T10:31:56.586Z","comments":false,"path":"tags/index - 副本.html","permalink":"https://www.chinawgs.cn/tags/index - 副本.html","excerpt":"","text":""},{"title":"books","date":"2019-06-02T12:08:24.224Z","updated":"2019-06-02T12:08:24.224Z","comments":false,"path":"books/index.html","permalink":"https://www.chinawgs.cn/books/index.html","excerpt":"","text":""}],"posts":[{"title":"贝叶斯估计","slug":"be","date":"2019-06-18T07:35:51.000Z","updated":"2019-06-18T10:53:53.094Z","comments":true,"path":"2019/06/18/be/","link":"","permalink":"https://www.chinawgs.cn/2019/06/18/be/","excerpt":"除了样本的信息，还有参数的先验信息。","text":"除了样本的信息，还有参数的先验信息。 例1. 抛硬币试验。 设正面朝上的概率为$\\theta$ ,假设未知参数$\\theta$的先验概率服从$\\beta$分布，记为$\\theta \\sim Be(\\alpha, \\beta)$，其概率密度是 p(\\theta)=\\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}不妨用$x_i=1$表示正面朝上，$x_i=0$表示反面朝上。抛硬币服从二项分布$X \\sim b(1,\\theta)$，即$X$的分布律为： P\\{X=x\\}=p(x;\\theta)=\\theta^x (1-\\theta)^{1-x}, x=0,1因为此时的$\\theta$为随机变量，所以$p(x;\\theta)$应该看成给定$\\theta$时$X$的条件概率，改用$p(x|\\theta)$来表示，即$X$的分布律为： P\\{X=x\\}=p(x|\\theta)=\\theta^x (1-\\theta)^{1-x}, x=0,1已知先验信息$p(\\theta)$，样本$X_1, X_2, \\cdots, X_n$取到观察值$x_1, x_2, \\cdots, x_n$， 参数$\\theta$的后验概率为： \\begin{align*} p(\\theta|x_1,x_2,\\cdots,x_n)&=\\frac{p(\\theta,x_1,x_2,\\cdots,x_n)}{p(x_1,x_2,\\cdots,x_n)}\\\\ & \\color{red} {注：联合密度除以边际密度}\\\\ &=\\frac{p(\\theta)p(x_1|\\theta)p(x_2|\\theta)\\cdots p(x_n|\\theta)}{\\int p(\\theta,x_1,x_2,\\cdots,x_n)d\\theta}\\\\ & \\color{red} {注：分母积分后与\\theta无关}\\\\ &= C \\theta^{\\alpha-1}(1- \\theta)^{\\beta-1}\\Pi_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}\\\\ & \\color{red} {注：C是一个与\\theta无关的常数}\\\\ &=C\\theta^{\\alpha -1 + \\Sigma_{i=1}^n x_i}(1-\\theta)^{\\beta-1+n- \\Sigma_{i=1}^n x_i} \\end{align*}令$L(\\theta)=p(\\theta|x_1,x_2, \\cdots, x_n)$ \\begin{align*} \\ln L(\\theta) &= \\ln C +\\ln[\\theta^{\\alpha -1 + \\Sigma_{i=1}^n x_i}(1-\\theta)^{\\beta-1+n- \\Sigma_{i=1}^n x_i}]\\\\ &= \\ln C +(\\alpha-1+\\sum_{i=1}^n x_i)\\ln\\theta + (\\beta-1+n-\\sum_{i=1}^n x_i)\\ln(1-\\theta) \\end{align*}令$\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta} =0$ \\frac{\\alpha-1+\\sum_{i=1}^n x_i}{\\theta}-\\frac{\\beta-1+n-\\sum_{i=1}^n x_i}{1-\\theta}=0\\\\ \\alpha-1+\\sum_{i=1}^n x_i=\\theta(n+\\alpha+\\beta-2)\\\\ \\hat \\theta = \\frac{\\alpha-1+\\sum_{i=1}^n x_i}{n+\\alpha+\\beta-2}当$n \\to \\infty$时，$\\hat \\theta \\to \\frac{1}{n} \\sum_{i=1}^n x_i= \\overline x$，此时正好是最大似然估计。 当$n=1$时，只有一个样本（$x_i=0或1$）， $\\hat \\theta=\\frac{\\alpha-1}{\\alpha+\\beta-1}$, ($x_i=0$)； $\\hat \\theta=\\frac{\\alpha}{\\alpha+\\beta-1}$, ($x_i=1$)。 例2. 推导下列正态分布均值$\\mu$的贝叶斯估计。 样本数据$x_1,x_2,\\cdots,x_n$来自正态分布$N(\\mu,\\sigma^2)$，其中$\\mu$未知，$\\sigma$已知。假设$\\mu$的先验分布为正态分布$\\mu \\sim N(0, \\tau^2)$ ，根据样本$x_1,x_2,\\cdots,x_n$写出$\\mu$的贝叶斯估计。 \\begin{align*} p(\\mu|x_1,x_2,\\cdots,x_n)&=\\frac{p(\\mu,x_1,x_2,\\cdots,x_n)}{p(x_1,x_2,\\cdots,x_n)}\\\\ &= \\frac{p(\\mu)p(x_1,x_2,\\cdots,x_n|\\mu)}{p(x_1,x_2,\\cdots,x_n)}\\\\ &= \\frac{p(\\mu)p(x_1|\\mu)p(x_2|\\mu)\\cdots p(x_n|\\mu)}{\\int p(\\mu,x_1,x_2,\\cdots,x_n) \\mathrm{d} \\mu}\\\\ &= \\frac{ \\frac{1}{\\sqrt{2\\pi}\\tau}\\exp(-\\frac{\\mu^2}{2\\tau^2})\\Pi_{i=1}^n\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}) } {\\int p(\\mu,x_1,x_2,\\cdots,x_n) \\mathrm{d} \\mu} \\\\ &= C \\exp(-\\frac{\\mu^2}{2\\tau^2}) \\Pi_{i=1}^n \\exp(-\\frac{(x_i-\\mu)^2}{2\\sigma^2})\\\\ & \\color{red} {注：C是一个与\\mu无关的常数}\\\\ \\end{align*} 令$L(\\mu)=p(\\mu|x_1,x_2,\\cdots,x_n)$ \\begin{align*} \\ln L(\\mu)=\\ln C - \\frac{\\mu^2}{2\\tau^2}-\\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2\\sigma^2} \\end{align*}令$ \\frac{\\partial \\ln L(\\mu)}{\\partial \\mu}=0$ -\\frac{\\mu}{\\tau^2}+\\sum_{i=1}^n \\frac{(x_i-\\mu)}{\\sigma^2}=0\\\\ \\tau^2 \\sum_{i=1}^n (x_i-\\mu)=\\sigma^2 \\mu\\\\ \\tau^2 \\sum_{i=1}^n x_i = (n \\tau^2 + \\sigma^2)\\mu\\\\ \\hat \\mu = \\frac{\\sum_{i=1}^n x_i}{n+ \\frac{\\sigma^2}{\\tau^2}}当$n \\to \\infty$时，$\\hat \\theta \\to \\frac{1}{n}\\sum_{i=1}^n x_i = \\overline x$，此时正好是最大似然估计。 总结： 问题：已知样本集$D=\\{x_1,x_2, \\cdots, x_n\\}$服从某个概率分布，但是参数$\\theta$未知。 最大似然估计 （1）参数$\\theta$是一个未知的定值 （2）目标：找到一个参数$\\theta$，使得样本集$D$发生的概率最大 \\max \\quad p(D|\\theta)贝叶斯估计 （1）参数$\\theta$是未知的随机变量，本身服从一定的概率分布（先验分布） （2）目标：样本集$D$发生的情况下，哪一个$\\theta$发生的概率最大 \\max \\quad p(\\theta|D) 贝叶斯估计求解的一般步骤： （1）确定参数$\\theta$的先验分布$p(\\theta)$ （2）由样本集$D=\\{x_1,x_2, \\cdots, x_n\\}$ 求出样本的联合分布$p(D|\\theta)$，它是$\\theta$的函数： p(D|\\theta)= \\Pi_{i=1}^n p(x_i|\\theta)（3）利用贝叶斯公式，求$\\theta$的后验分布： L(\\theta)=p(\\theta|D)=\\frac{p(\\theta,D)}{p(D)}=\\frac{p(D|\\theta)p(\\theta)}{\\int p(\\theta,D)\\mathrm{d} \\theta}（4）取对数 $\\ln L(\\theta)$ （5）求偏导 $\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta}$ （6）解方程（组） $\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta}=0$，求解出$\\hat \\theta$ 参考文献： [1] 邰淑彩, 孙韫玉, 何娟娟. 应用数理统计（第二版）[M]. 武汉: 武汉大学出版社, 2005.","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.chinawgs.cn/categories/学习笔记/"}],"tags":[{"name":"概率统计","slug":"概率统计","permalink":"https://www.chinawgs.cn/tags/概率统计/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.chinawgs.cn/tags/机器学习/"}],"author":"吴光生"},{"title":"贝叶斯公式","slug":"bayes","date":"2019-06-11T06:47:06.000Z","updated":"2019-06-11T13:52:49.530Z","comments":true,"path":"2019/06/11/bayes/","link":"","permalink":"https://www.chinawgs.cn/2019/06/11/bayes/","excerpt":"条件概率条件概率的定义：设$A$, $B$是两个事件，且$P(A)&gt;0$，称 P(B|A)=\\frac{P(AB)}{P(A)}为在事件$A$发生的条件下，事件$B$发生的条件概率。","text":"条件概率条件概率的定义：设$A$, $B$是两个事件，且$P(A)&gt;0$，称 P(B|A)=\\frac{P(AB)}{P(A)}为在事件$A$发生的条件下，事件$B$发生的条件概率。 同理，可得出在事件$B$发生的条件下，事件$A$发生的条件概率为 P(A|B)=\\frac{P(AB)}{P(B)} 根据文氏图可以看出，在事件$A$发生的情况下，事件$B$发生的概率，就是$P(A \\cap B)$除以$P(A)$，即$P(AB)$除以$P(A)$。 乘法定理设$P(A) &gt; 0$，则有 P(AB)=P(B|A)P(A)由条件概率的定义，可得到上述乘法定理。 同理，可得： P(AB)=P(A|B)P(B) $P(AB)$又叫联合概率（两个事件共同发生的概率），还可以记为$P(A \\cap B)$或者$P(A,B)$ 样本空间及其划分定义：设$S$为试验$E$的样本空间，$B_1,B_2,\\cdots,B_n$为$E$的一组事件。若 (1) $B_i B_j = \\emptyset$， $i \\neq j$， $ i,j=1,2, \\cdots, n$； (2) $B_1 \\cup B_2 \\cup \\cdots \\cup B_n = S$， 则称$B_1 , B_2 , \\cdots , B_n $为样本空间$S$的一个划分。 若$B_1 , B_2 , \\cdots , B_n $为样本空间$S$的一个划分，那么，对于每次试验，事件$B_1 , B_2 , \\cdots , B_n $中必有一个且仅有一个发生。 全概率公式定理：设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1 , B_2 , \\cdots , B_n$ 为$S$的一个划分，且$P(B_i)&gt;0$ （$i=1,2, \\cdots, n$），则 P(A)=P(A|B_1)P(B_1)+P(A|B_2)P(B_2)+ \\cdots +P(A|B_n)P(B_n)称为全概率公式。 特别地，当$n=2$，并将$B_1$记为$B$，此时$B_2$就是$\\overline B$， 全概率公式变为 P(A)=P(A|B)P(B)+P(A|\\overline B)P(\\overline B) 贝叶斯公式定理：设试验$E$的样本空间为$S$，$A$为$E$的事件，$B_1 , B_2 , \\cdots , B_n$ 为$S$的一个划分，且$P(A)&gt;0$，$P(B_i)&gt;0$ （$i=1,2, \\cdots, n$），则 P(B_i|A)=\\frac{P(A|B_i)P(B_i)}{\\sum_{j=1}^n P(A|B_j)P(B_j)}, \\quad i=1,2, \\cdots,n称为贝叶斯公式。 证明：由条件概率的定义及全概率公式即得 P(B_i|A)=\\frac{P(B_iA)}{P(A)}=\\frac{P(A|B_i)P(B_i)}{\\sum_{j=1}^n P(A|B_j)P(B_j)}, \\quad i=1,2, \\cdots,n 特别地，当$n=2$，并将$B_1$记为$B$，此时$B_2$就是$\\overline B$， 贝叶斯公式变为 P(B|A)=\\frac{P(AB)}{P(A)}=\\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\\overline B)P(\\overline B)}其中，$P(B)$称为$B$的先验概率，即在事件$A$发生之前，我们对$B$事件发生概率的一个基本判断，往往是基于以往的数据分析得到的。$P(B|A)$称为$B$的后验概率，即在$A$事件发生之后，我们对$B$事件发生的概率进行重新评估。 $A$和$B$只是表示事件的符号而已，类似地，贝叶斯公式也可被写为： P(A|B)=\\frac{P(AB)}{P(B)}=\\frac{P(B|A)P(A)}{P(B|A)P(A)+P(B|\\overline A)P(\\overline A)}此时，$P(A)$称为$A$的先验概率，即在事件$B$发生之前，我们对$A$事件发生概率的一个基本判断，往往是基于以往的数据分析得到的。$P(A|B)$称为$A$的后验概率，即在$B$事件发生之后，我们对$A$事件发生的概率进行重新评估。 举例例1. 已知人群中某种疾病的发病率为0.001。现有一种试剂可以检验是否患有该疾病，在患病情况下，有99%的可能呈现阳性。没患病情况下，也有5%的可能呈现阳性。现有一个人被检出阳性，请问他确实患有该病的可能性有多大？ 解：以$A$表示事件“检验结果呈现阳性”，$B$表示事件“患病”。 已知$P(B)=0.001$，这是先验概率，即没有检查之前，预计的患病可能性，根据人群的历史发病数据得来的。 现在要求$P(B|A)$，就是后验概率，即检查为阳性之后患病的可能性。 \\begin{align*} P(B|A) &= \\frac{P(AB)}{P(A)} \\\\ &= \\frac{P(A|B)P(B)}{P(A|B)P(B)+P(A|\\overline B)P(\\overline B)} \\\\ &= \\frac{0.99 \\times 0.001}{0.99 \\times 0.001 + 0.05 \\times 0.999} \\\\ & \\approx 0.019 \\end{align*}参考文献： [1] 盛骤, 谢式千, 潘承毅. 概率论与数理统计（第四版）[M]. 北京: 高等教育出版社, 2008.","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.chinawgs.cn/categories/学习笔记/"}],"tags":[{"name":"概率统计","slug":"概率统计","permalink":"https://www.chinawgs.cn/tags/概率统计/"}],"author":"吴光生"},{"title":"主成分分析（PCA）","slug":"pca","date":"2019-06-04T15:10:06.000Z","updated":"2019-06-04T16:01:38.304Z","comments":true,"path":"2019/06/04/pca/","link":"","permalink":"https://www.chinawgs.cn/2019/06/04/pca/","excerpt":"1. 主成分分析优点：降低数据的复杂性，识别最重要的多个特征。 缺点：不一定需要，且可能损失有用信息。 使用数据类型：数值型数据","text":"1. 主成分分析优点：降低数据的复杂性，识别最重要的多个特征。 缺点：不一定需要，且可能损失有用信息。 使用数据类型：数值型数据 2. 在NumPy中实现PCA将数据转换成前K个主成分的伪码： 去除平均值（让样本矩阵中心化，即每一个维度减去该维度的均值，使每一维度上的均值为0） 计算协方差矩阵 计算协方差矩阵的特征值和特征向量 将特征值从大到小排序 保留最上面的K个特征向量 将数据转换到上述K个特征向量构建的新空间中 补充阅读：协方差矩阵 （1）一个维度上方差的定义： var(X)=\\frac{\\sum_{i=1}^{n}(X_{i}-{\\bar{X}})(X_{i}-{\\bar{X}})}{n-1}（2）协方差的定义： cov(X,Y)=\\frac{\\sum_{i=1}^{n}(X_{i}-{\\bar{X}})(Y_{i}-{\\bar{Y}})}{n-1}方差是协方差的一个特例，X与其自身的协方差就是X的方差。 分母为n-1是因为随机变量的数学期望未知，以样本均值代替，自由度减一。 协方差就是定义了两个维度之间的相关性，即这个样本的两个维度之间有没有关系。 协方差为0，证明这两个维度之间没有关系；协方差为正，两个维度正相关；协方差为负，两个维度负相关。 （3）协方差矩阵： 对n个维度，任意两个维度都计算一个协方差，组成矩阵。 C_{n \\times n}=( c_{i,j}, c_{i,j}=cov(Dim_{i},Dim_{j}))直观地，对于一个含有x，y，z三个维度的样本，协方差矩阵如下： C=\\begin{pmatrix} cov(x,x) & cov(x,y) & cov(x,z)\\\\ cov(y,x) & cov(y,y) & cov(y,z)\\\\ cov(z,x) & cov(z,y) & cov(z,z) \\end{pmatrix}可以看出，对角线表示了样本在各个维度上的方差，其他元素表示不同维度两两之间的相关性。 例： 假设原始数据为5个样本，每个样本有2个特征x和y。矩阵是： Data = \\begin{bmatrix} 1 & -1\\\\ 1 & 1\\\\ 2 & 1\\\\ 2 & 2\\\\ 4 & 2 \\end{bmatrix}第一步，样本矩阵中心化（计算每个维度的均值，所有样本都减去均值） 维度x的均值为2，维度y的均值为1。 各维度去均值后，矩阵是： DataAdjust = \\begin{bmatrix} -1 & -2\\\\ -1 & 0\\\\ 0 & 0\\\\ 0 & 1\\\\ 2 & 1 \\end{bmatrix}这时，两个维度上的均值都为0。 第二步，计算协方差矩阵 协方差矩阵： \\begin{align*} C &= \\begin{bmatrix} cov(x,x) & cov(x,y)\\\\ cov(y,x) & cov(y,y) \\end{bmatrix}\\\\ &= \\begin{bmatrix} \\frac{(-1) \\times (-1) + (-1) \\times (-1) + 0 \\times 0 + 0 \\times 0 + 2 \\times 2}{4} & \\frac{(-1) \\times (-2) + (-1) \\times (0) + 0 \\times 0 + 0 \\times 1 + 2 \\times 1}{4}\\\\ \\frac{(-2) \\times (-1) + (0) \\times (-1) + 0 \\times 0 + 1 \\times 0 + 1 \\times 2}{4} & \\frac{(-2) \\times (-2) + (0) \\times (0) + 0 \\times 0 + 1 \\times 1 + 1 \\times 1}{4} \\end{bmatrix}\\\\ &= \\begin{bmatrix} 1.5 & 1.0\\\\ 1.0 & 1.5 \\end{bmatrix} \\end{align*}第三步，计算协方差矩阵的特征值和特征向量 eigenValues=\\begin{pmatrix} 2.5 & 0.5 \\end{pmatrix} eigenVectors=\\begin{pmatrix} 0.70710678 & -0.70710678\\\\ 0.70710678 & 0.70710678 \\end{pmatrix}特征值2.5对应的特征向量是eigenVectors的第1列： \\begin{pmatrix} 0.70710678\\\\ 0.70710678 \\end{pmatrix}特征值0.5对应的特征向量是eigenVectors的第2列： \\begin{pmatrix} -0.70710678\\\\ 0.70710678 \\end{pmatrix}第四步，将特征值按照从大到小的顺序排列，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵 这里特征值只有2个，我们选取最大的那个2.5，对应的特征向量是： \\begin{pmatrix}0.70710678\\\\0.70710678\\end{pmatrix}第五步，将样本点投影到选取的特征向量上。 假设样本数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m×n)，协方差矩阵为C(n×n)，选取的k个特征向量组成的矩阵为TopEigenVectors(n×k)，那么投影后的低维数据LowDimData为： LowDimData_{m\\times k}=DataAdjust_{m \\times n} \\times TopEigenVectors_{n \\times k}这里是 LowDimData= \\begin{bmatrix} -1 & -2\\\\ -1 & 0\\\\ 0 & 0\\\\ 0 & 1\\\\ 2 & 1 \\end{bmatrix} \\times \\begin{pmatrix} 0.70710678\\\\ 0.70710678 \\end{pmatrix} = \\begin{bmatrix} -2.12132034\\\\ -0.70710678\\\\ 0\\\\ 0.70710678\\\\ 2.12132034 \\end{bmatrix}这样就将原始样本的n维（这个例子n=2）特征变成了k维（这个例子k=1），这k维就是原始特征在k维上的投影。 第六步，查看重构之后的数据，用于调试（假设K取值为n，重构回去的数据reconMat应该与原始数据Data重合） reconMat = (LowDimData * TopEigenVectors.T) + meanVals程序清单 PCA算法 from numpy import * import matplotlib import matplotlib.pyplot as plt def loadDataSet(fileName, delim=&#39;\\t&#39;): fr = open(fileName) stringArr = [line.strip().split(delim) for line in fr.readlines()] datArr = [list(map(float,line)) for line in stringArr] # python3 needs list() return mat(datArr) def pca(dataMat, topNfeat=9999999): meanVals = mean(dataMat, axis=0) dataAdjust = dataMat - meanVals #remove mean covMat = cov(dataAdjust, rowvar=0) eigVals,eigVects = linalg.eig(mat(covMat)) eigValInd = argsort(eigVals) #sort, sort goes smallest to largest eigValInd = eigValInd[:-(topNfeat+1):-1] #cut off unwanted dimensions topEigVects = eigVects[:,eigValInd] #reorganize eig vects largest to smallest lowDimDataMat = dataAdjust * topEigVects#transform data into new dimensions reconMat = (lowDimDataMat * topEigVects.T) + meanVals return lowDimDataMat, reconMat dataMat = loadDataSet(&#39;testSet.txt&#39;) lowDimDataMat, reconMat = pca(dataMat, 1) fig = plt.figure() ax = fig.add_subplot(111) ax.scatter(dataMat[:,0].flatten().A[0], dataMat[:,1].flatten().A[0], marker=&#39;^&#39;, s=90) ax.scatter(reconMat[:,0].flatten().A[0], reconMat[:,1].flatten().A[0], marker=&#39;o&#39;, s=50, c=&#39;red&#39;) plt.show() 运行结果如图1： 图1 原始数据集(三角形)及第一主成分(圆形) 用到的数据集testSet.txt 参考文献： [1] (美) Harrington Peter著. 机器学习实战[M]. 李锐, 李鹏, 曲亚东, 王斌, 译. 北京: 人民邮电出版社, 2013.","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.chinawgs.cn/categories/学习笔记/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://www.chinawgs.cn/tags/机器学习/"}],"author":"吴光生"},{"title":"最大似然估计(MLE)","slug":"mle","date":"2019-06-02T10:23:12.000Z","updated":"2019-06-14T08:48:13.912Z","comments":true,"path":"2019/06/02/mle/","link":"","permalink":"https://www.chinawgs.cn/2019/06/02/mle/","excerpt":"什么是最大似然估计？最大似然估计（Maximum Likelihood Estimation）是一种参数估计的方法。其核心思想是：找到参数$\\theta$的一个估计值，使得当前样本出现的可能性最大。","text":"什么是最大似然估计？最大似然估计（Maximum Likelihood Estimation）是一种参数估计的方法。其核心思想是：找到参数$\\theta$的一个估计值，使得当前样本出现的可能性最大。 设总体$X$属于离散型，其分布律$P\\{ X=x\\}=p(x;\\theta), \\theta \\in \\Theta$的形式为已知，$\\theta$为待估参数，$\\Theta$是$\\theta$可能取值的范围。设$X_1,X_2,\\cdots,X_n$是来自$X$的样本，则$X_1,X_2,\\cdots,X_n$的联合分布律为： \\prod_{i=1}^{n}p(x_i;\\theta)样本$X_1,X_2,\\cdots,X_n$取到观察值$x_1,x_2,\\cdots,x_n$的概率，即事件$\\{X_1=x_1,X_2=x_2,\\cdots,X_n=x_n\\}$发生的概率为： L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^{n}p(x_i;\\theta),\\theta\\in\\Theta这一概率随$\\theta$的取值而变化，它是$\\theta$的函数，$L(\\theta)$称为样本的似然函数。 最大似然估计法，就是固定样本观察值$x_1,x_2,\\cdots,x_n$，在参数$\\theta$取值的可能范围$\\Theta$内挑选出一个$\\hat\\theta$，使得似然函数$L(x_1,x_2,\\cdots,x_n;\\theta)$达到最大。即： \\hat\\theta=\\mathop{\\arg\\max}_{\\theta\\in\\Theta}L(x_1,x_2,\\cdots,x_n;\\theta)这样得到的$\\hat\\theta$与样本值$x_1,x_2,\\cdots,x_n$有关，常记为$\\hat\\theta(x_1,x_2,\\cdots,x_n)$，称为参数$\\theta$的最大似然估计值，而相应的统计量$\\hat\\theta(X_1,X_2,\\cdots,X_n)$称为参数$\\theta$的最大似然估计量。 为什么要有参数估计？当模型已定，参数未知时。 例如，假设我们知道全国人民的身高服从正态分布，但不知道均值和方差。这时可以通过采样，观察其结果，然后再用样本数据的结果推出正态分布的均值与方差的最大概率值，这样就可以知道全国人民的身高分布的函数。 举例1. 抛硬币。现有一个正反面不是很均匀的硬币，如果正面朝上记为H，反面朝上记为T，抛10次的结果如下： T, H, T, T, H, T, T, T, H, T 求这个硬币正面朝上的概率有多大？ 很显然，这个概率是0.3。现在用MLE的思想来求解它。 设$x_1,x_2,\\cdots,x_n$是相应于样本$X_1,X_2,\\cdots,X_n$的一个样本值。 不妨用$x_i=1$表示正面朝上，$x_i=0$表示反面朝上 设正面朝上的概率为$\\theta$，抛硬币服从二项分布$X \\sim b(1,\\theta)$，即$X$的分布律为： P\\{X=x\\}=p(x;\\theta)=\\theta^x (1-\\theta)^{1-x}, x=0,1 似然函数为： L(\\theta)=\\prod_{i=1}^{n}p(x_i;\\theta)=\\prod_{i=1}^{n}\\theta^{x_i}(1-\\theta)^{1-x_i} 取对数后，为 \\begin{align*} \\ln L(\\theta)&=\\ln \\prod_{i=1}^{n}\\theta^{x_i}(1-\\theta)^{1-x_i}\\\\ &=\\sum_{i=1}^{n}\\ln[\\theta^{x_i}(1-\\theta)^{1-x_i}]\\\\ &=\\sum_{i=1}^{n}[\\ln \\theta^{x_i}+\\ln(1-\\theta)^{1-x_i}]\\\\ &=\\sum_{i=1}^{n}[x_i \\ln \\theta+(1-x_i)\\ln(1-\\theta)]\\\\ &=\\sum_{i=1}^{n}x_i \\ln \\theta+(n-\\sum_{i=1}^{n}x_i)\\ln(1-\\theta) \\end{align*} 求导： \\frac{\\partial \\ln L(\\theta)}{\\partial \\theta}=\\frac{\\sum_{i=1}^{n} x_i}{\\theta}-\\frac{n-\\sum_{i=1}^{n} x_i}{1-\\theta} 令$\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta}=0$，可得： \\hat \\theta = \\frac{\\sum_{i=1}^{n} x_i}{n} 可知概率$\\hat \\theta=0.3$ 2. 设$X \\sim N(\\mu, \\sigma^2)$, $\\mu, \\sigma^2$为未知参数，$x_1,x_2,\\cdots,x_n$是来自$X$的一个样本值。求$\\mu, \\sigma^2$的最大似然估计量。 解：$X$的概率密度为： f(x;\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{1}{2\\sigma^2}(x-\\mu)^2] 似然函数为： \\begin{align*} L(\\mu,\\sigma^2)&=\\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}\\sigma}exp[-\\frac{1}{2\\sigma^2}(x_i-\\mu)^2]\\\\ &=(\\frac{1}{\\sqrt{2\\pi}\\sigma})^n exp[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2]\\\\ &=(\\frac{1}{2\\pi \\sigma^2})^{\\frac{n}{2}} exp[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2]\\\\ &=(2\\pi)^{-\\frac{n}{2}}(\\sigma^2)^{-\\frac{n}{2}}exp[-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2] \\end{align*} 它的对数： \\ln L(\\mu,\\sigma^2)=-\\frac{n}{2}\\ln(2\\pi)-\\frac{n}{2}\\ln(\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (x_i-\\mu)^2 令 \\left\\{ \\begin{array}{l} \\frac{\\partial \\ln L(\\mu,\\sigma^2)}{\\partial \\mu}=\\frac{1}{\\sigma^2}\\Sigma_{i=1}^n(x_i-\\mu)=0\\\\ \\frac{\\partial \\ln L(\\mu,\\sigma^2)}{\\partial \\sigma^2}= -\\frac{n}{2\\sigma^2} + \\frac{1}{2 \\sigma^4}\\sum_{i=1}^n (x_i-\\mu)^2=0 \\end{array} \\right. 联合求解，得到参数$\\mu$和$\\sigma^2$的最大似然估计值分别为： \\left\\{ \\begin{array}{l} \\hat \\mu = \\overline x = \\frac{1}{n} \\sum_{i=1}^n x_i\\\\ \\hat \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i-\\overline x)^2 \\end{array} \\right. 相应的最大似然估计量分别为： $\\hat \\mu=\\bar X$, $\\hat \\sigma^2=\\frac{1}{n}\\sum_{i=1}^n (X_i- \\overline X)$ 总结求最大似然估计值的一般步骤： （1）写出总体$X$的分布律$p(x; \\theta)$（$X$为离散型随机变量）或者概率密度$f(x; \\theta)$（$X$为连续型随机变量） （2）写出样本的似然函数$L(\\theta)$ 离散型：$L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^{n}p(x_i;\\theta)$连续型：$L(\\theta)=L(x_1,x_2,\\cdots,x_n;\\theta)=\\prod_{i=1}^{n}f(x_i;\\theta)$ （3）对似然函数取对数 $\\ln L(\\theta)$ （4）求偏导 $\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta}$ （5）解方程（组） $\\frac{\\partial \\ln L(\\theta)}{\\partial \\theta}=0$，得到参数$\\theta$的最大似然估计值$\\hat \\theta$ 注：参数可能是一个（如例1，只有一个参数$\\theta$），也可能是一组（如例2，有两个参数：$\\mu, \\sigma^2$），一组参数时，求解方法类似。 参考文献： [1] 盛骤, 谢式千, 潘承毅. 概率论与数理统计（第四版）[M]. 北京: 高等教育出版社, 2008.","categories":[{"name":"学习笔记","slug":"学习笔记","permalink":"https://www.chinawgs.cn/categories/学习笔记/"}],"tags":[{"name":"概率统计","slug":"概率统计","permalink":"https://www.chinawgs.cn/tags/概率统计/"},{"name":"机器学习","slug":"机器学习","permalink":"https://www.chinawgs.cn/tags/机器学习/"}],"author":"吴光生"},{"title":"Hello World","slug":"hello-world","date":"2019-06-02T05:20:10.000Z","updated":"2019-06-03T05:52:32.144Z","comments":true,"path":"2019/06/02/hello-world/","link":"","permalink":"https://www.chinawgs.cn/2019/06/02/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[{"name":"测试","slug":"测试","permalink":"https://www.chinawgs.cn/categories/测试/"}],"tags":[{"name":"杂谈","slug":"杂谈","permalink":"https://www.chinawgs.cn/tags/杂谈/"}]}]}